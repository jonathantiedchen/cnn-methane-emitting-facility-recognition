{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data to execute this Notebook cannot be provided due to file size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwing cell to pip install the necerssary packages specified in the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions \n",
    "\n",
    "- Training Accuracy and Loss Graphs\n",
    "- Predictions                \n",
    "- True and Predicted Classes \n",
    "- Plot metrices (Accuracy, F1 Score, Recall, Precision)         \n",
    "- Confusion Matrix (as array)          \n",
    "- Plot Confusion Matrix  \n",
    "- Plot image with index\n",
    "- Plot image with prediction with index\n",
    "- Plot wrong predicted images\n",
    "- Plot correct predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "channels=3\n",
    "autotune = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "\n",
    "\n",
    "def plot_history(model):\n",
    "    \"\"\"Plots the accuracy and loss of the inputted model.\"\"\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['accuracy'])\n",
    "    plt.plot(model.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_history_vgg(model):\n",
    "    \"\"\"Plots the accuracy and loss of the inputted model.\"\"\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history.history['accuracy'])\n",
    "    plt.plot(model.history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history.history['loss'])\n",
    "    plt.plot(model.history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def create_labels (labels):\n",
    "    y_labels = []\n",
    "    for label in labels: \n",
    "        y_labels.append(label)\n",
    "    y_labels = np.array(y_labels)\n",
    "    return y_labels\n",
    "\n",
    "\n",
    "def true_and_predicted_classes(predictions, labels):\n",
    "    predicted_classes = []\n",
    "    true_classes = []\n",
    "    for prediction in predictions:  \n",
    "        pred = np.argmax(prediction)\n",
    "        predicted_classes.append(pred)\n",
    "    for label in labels: \n",
    "        true = np.argmax(label)\n",
    "        true_classes.append(true)\n",
    "    return predicted_classes, true_classes\n",
    "\n",
    "\n",
    "def print_predictions(labels, preds, predictions):\n",
    "    \"\"\"Predictions based on test dataset.\"\"\"\n",
    "    #predict\n",
    "    for label,pred, prediction in zip(labels, preds, predictions):\n",
    "        \n",
    "            print(\"Prediction:\", prediction,\"Pred. Class: \",pred, \"Actual Label:\", label)# Print the first prediction\n",
    "        \n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "    # Assuming accuracy was the second metric (index 1), extract the accuracy.\n",
    "    test_accuracy = result[1] * 100  # Convert to percentage\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def print_conf_matrix(true_classes, predicted_classes):\n",
    "    \"\"\"\n",
    "    Print confusion matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "    df_cm = pd.DataFrame(\n",
    "        conf_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "    # Set aesthetics for better readability\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_image(index, images, labels):\n",
    "    \"\"\"\n",
    "    Plots a single image with its predicted and actual labels.\n",
    "    \"\"\"\n",
    "    # Get a single image from X_val\n",
    "    image = images[index]\n",
    "\n",
    "    # Get the true label for the image\n",
    "    y_true = np.argmax(labels[index])\n",
    "\n",
    "    # Plot the image \n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap='bone')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(f\"Label: {class_names[y_true]}\")\n",
    "    plt.title(\"Image with Predicted and Actual Labels\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_image_pred(model, index, images, labels):\n",
    "    \"\"\"\n",
    "    Plots a single image with its predicted and actual labels.\n",
    "    \"\"\"\n",
    "    # Get a single image from X_val\n",
    "    image = images[index]\n",
    "\n",
    "    # Get the true label for the image\n",
    "    y_true = np.argmax(labels[index])\n",
    "\n",
    "    # Predict the label using the model\n",
    "    y_pred = np.argmax(model.predict(np.expand_dims(image, axis=0)))\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap='bone')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(f\"True: {class_names[y_true]}\\nPred: {class_names[y_pred]}\",\n",
    "               color='black' if y_true == y_pred else 'red')\n",
    "    plt.title(\"Image with Predicted and Actual Labels\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_wrong_pred_image(images, true_classes, predicted_classes):\n",
    "    \"\"\"\n",
    "    Plots a single image with its predicted and actual labels.\n",
    "    \"\"\"\n",
    "    mismatch =  [i for i in range(len(true_classes)) if true_classes[i] != predicted_classes[i]]\n",
    "    random_nr = random.choice(mismatch)\n",
    "    # Get a single image from X_val\n",
    "    image = images[random_nr]\n",
    "\n",
    "    # Get the true label for the image\n",
    "    y_true = true_classes[random_nr]\n",
    "\n",
    "    # Predict the label using the model\n",
    "    y_pred = predicted_classes[random_nr]\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap='bone')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(f\"True: {class_names[y_true]}\\nPred: {class_names[y_pred]}\",\n",
    "               color='black' if y_true == y_pred else 'red')\n",
    "    plt.title(\"Wrong predicted image\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_correct_pred_image(images, true_classes, predicted_classes):\n",
    "    \"\"\"\n",
    "    Plots a single image with its predicted and actual labels.\n",
    "    \"\"\"\n",
    "    mismatch =  [i for i in range(len(true_classes)) if true_classes[i] == predicted_classes[i]]\n",
    "    random_nr = random.choice(mismatch)\n",
    "    # Get a single image from X_val\n",
    "    image = images[random_nr]\n",
    "\n",
    "    # Get the true label for the image\n",
    "    y_true = true_classes[random_nr]\n",
    "\n",
    "    # Predict the label using the model\n",
    "    y_pred = predicted_classes[random_nr]\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap='bone')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(f\"True: {class_names[y_true]}\\nPred: {class_names[y_pred]}\")\n",
    "    plt.title(\"Correct predicted image\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrices(true_classes,predicted_classes):\n",
    "    #print evaluation metrices\n",
    "    print(f\"Accuracy Score: {accuracy_score(true_classes, predicted_classes)}\")\n",
    "    print(f\"F1 Score: {f1_score(true_classes, predicted_classes, average='weighted')}\")\n",
    "    print(f\"Recall Score: {recall_score(true_classes, predicted_classes, average='weighted')}\")\n",
    "    print(f\"Precision Score: {precision_score(true_classes, predicted_classes, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Paths and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv files to get filepaths and labels \n",
    "df_train= pd.read_csv(\"data/FINAL_METER_ML_train_2000.csv\")\n",
    "df_val = pd.read_csv(\"data/FINAL_METER_ML_val.csv\")\n",
    "df_test = pd.read_csv(\"data/FINAL_METER_ML_test.csv\")\n",
    "\n",
    "# convert each string in the DataFrame to a list\n",
    "df_train['Label'] = df_train['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "df_val['Label'] = df_val['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "df_test['Label'] = df_test['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "\n",
    "\n",
    "#define class names for later use\n",
    "class_names=[\"CAFOs\",\"Landfills\",\"Mines\",\"Negative\",\"ProcessingPlants\",\"RefineriesAndTerminals\",\"WWTreatment\"]\n",
    "\n",
    "#convert the label labels to arrays in a list\n",
    "y_train = np.array(df_train['Label']).tolist()\n",
    "y_val = np.array(df_val['Label']).tolist()\n",
    "y_test = np.array(df_test['Label']).tolist()\n",
    "\n",
    "#import images from previous created .npy files\n",
    "X_val = np.load('data/x_val.npy')\n",
    "X_test = np.load('data/x_test.npy')\n",
    "X_train = np.load('data/x_train.npy')\n",
    "\n",
    "#create labels\n",
    "y_val = create_labels(y_val)\n",
    "y_train = create_labels(y_train)\n",
    "y_test = create_labels(y_test)\n",
    "\n",
    "class_names=[\"CAFOs\",\"Landfills\",\"Mines\",\"Negative\",\"ProcessingPlants\",\"RefineriesAndTerminals\",\"WWTreatment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50: \n",
    "- https://datagen.tech/guides/computer-vision/resnet-50/\n",
    "- https://medium.com/@bravinwasike18/building-a-deep-learning-model-with-keras-and-resnet-50-9dd6f4eb3351\n",
    "- https://medium.com/@ozgunhaznedar/image-classification-on-satellite-images-with-deep-learning-baa9813dde4e\n",
    "- https://wandb.ai/mostafaibrahim17/ml-articles/reports/The-Basics-of-ResNet50---Vmlldzo2NDkwNDE2#step-4:-building-resnet-50-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables\n",
    "train_epochs = 30\n",
    "tune_epochs = 10\n",
    "total_epochs = train_epochs + tune_epochs\n",
    "batch_size = 128\n",
    "\n",
    "# import resnet model for transfer learning\n",
    "rn50_base = tf.keras.applications.ResNet50(\n",
    "    include_top = False,\n",
    "    weights = \"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    "    )\n",
    "\n",
    "# Freeze layers pf basemodel, so the pre-trained weights are fixed\n",
    "for each_layer in rn50_base.layers:\n",
    "        each_layer.trainable=False\n",
    "\n",
    "# create sequential model\n",
    "resnet_model = Sequential()\n",
    "\n",
    "# Add output layers for finetuning\n",
    "resnet_model.add(rn50_base)\n",
    "resnet_model.add(Flatten()) #use flatten instead of GlobalAveragePooling2D as it may yield better results when enough data\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "resnet_model.compile(optimizer= tf.keras.optimizers.Adam(0.0001),\n",
    "                    loss= tf.keras.losses.CategoricalCrossentropy(), \n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "#initializt time\n",
    "t0 = time.time()\n",
    "\n",
    "# Train model \n",
    "history = resnet_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        validation_data = (X_val,y_val), \n",
    "                        epochs = train_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks = [ReduceLROnPlateau(patience=5), EarlyStopping(patience=5)])\n",
    "t1 = time.time()\n",
    "\n",
    "#print time in seconds\n",
    "print(\"Training time in seconds:\", t1-t0)\n",
    "#save model\n",
    "#resnet_model.save(\"resnet_model.keras\")\n",
    "\n",
    "# Plot model information\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreeze last convolution layer for fine tuning\n",
    "for each_layer in rn50_base.layers:\n",
    "        each_layer.trainable=False\n",
    "for layer in [l for l in rn50_base.layers if 'conv5' in l.name]:\n",
    "   layer.trainable = True\n",
    "\n",
    "#check if last concolution block has Trainable = True\n",
    "for i, layer in enumerate(rn50_base.layers):\n",
    "    print(i, layer.name, \"-\", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with smaller learning rate\n",
    "resnet_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001),\n",
    "                    loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "#initialize timing\n",
    "t0 = time.time()\n",
    "\n",
    "# Train model \n",
    "history = resnet_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        validation_data = (X_val,y_val), \n",
    "                        epochs=total_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks = [ReduceLROnPlateau(patience=5), EarlyStopping(patience=5)])\n",
    "t1 = time.time()\n",
    "print(\"Training time in seconds:\", t1-t0)\n",
    "\n",
    "#save trained model\n",
    "#resnet_model.save(\"resnet_model_tf.keras\")\n",
    "\n",
    "# Plot training plots\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "alexnet = Sequential()\n",
    "\n",
    "alexnet.add(Conv2D(96, kernel_size=(11,11), strides= 4,\n",
    "                        padding= 'valid', activation= 'relu',\n",
    "                        input_shape= (224,224,3),\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'valid', data_format= None))\n",
    "\n",
    "alexnet.add(Conv2D(256, kernel_size=(5,5), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "                        \n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'valid', data_format= None)) \n",
    "\n",
    "alexnet.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "alexnet.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "alexnet.add(Conv2D(256, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                        padding= 'valid', data_format= None))\n",
    "\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(4096, activation= 'relu'))\n",
    "alexnet.add(Dense(4096, activation= 'relu'))\n",
    "\n",
    "#Output layer with 7 classes insteas of 1000 as in model architecture:\n",
    "alexnet.add(Dense(7, activation= 'softmax'))\n",
    "\n",
    "\n",
    "# Compile the model with lower learning rate than Adam default:\n",
    "alexnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "#initialize timing\n",
    "t0=time.time()\n",
    "\n",
    " #Fit model and save history for further analysis:\n",
    "history = alexnet.fit(X_train,\n",
    "                      y_train,\n",
    "                      validation_data = (X_val, y_val), \n",
    "                      epochs=30,\n",
    "                      batch_size=128,\n",
    "                      callbacks=[tf.keras.callbacks.EarlyStopping(patience=5), tf.keras.callbacks.ReduceLROnPlateau(patience=5)])\n",
    "\n",
    "print(\"Training time in seconds:\", time.time()-t0)\n",
    "\n",
    "#save model\n",
    "#alexnet.save('alexnet.keras')\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Variables\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Define Model\n",
    "_input = Input((224,224,3)) \n",
    "\n",
    "conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n",
    "conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "pool1  = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "pool2  = MaxPooling2D((2, 2))(conv4)\n",
    "\n",
    "conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "pool3  = MaxPooling2D((2, 2))(conv7)\n",
    "\n",
    "conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "pool4  = MaxPooling2D((2, 2))(conv10)\n",
    "\n",
    "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
    "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
    "pool5  = MaxPooling2D((2, 2))(conv13)\n",
    "\n",
    "flat   = Flatten()(pool5)\n",
    "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
    "output = Dense(7, activation=\"softmax\")(dense2) #adapted number of outputs and outputfunction\n",
    "\n",
    "vgg16_model  = Model(inputs=_input, outputs=output)\n",
    "\n",
    "\n",
    "#compile the model with lower learing rate than adama default:\n",
    "vgg16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "##provide a model summary\n",
    "vgg16_model.summary()\n",
    "\n",
    "#initialize timing\n",
    "t0=time.time()\n",
    "\n",
    "#fit the model\n",
    "history = vgg16_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks = [ReduceLROnPlateau(patience=5), EarlyStopping(patience=5)])\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Training time in seconds:\", t1-t0)\n",
    "\n",
    "# save model:\n",
    "#vgg16_model.save(\"vgg16_model.keras\")\n",
    "\n",
    "#Plot history\n",
    "plot_history_vgg(vgg16_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict test set\n",
    "predictions_vgg = vgg16_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy of test predictions\n",
    "evaluate_model(vgg16_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get true classes and predicted classes in lists\n",
    "true_classes, predicted_classes = true_and_predicted_classes(predictions_vgg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print evaluation metrices\n",
    "plot_metrices(true_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix\n",
    "print_conf_matrix(true_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the classifiaction report - evaluation metrices accross the different classes\n",
    "report = classification_report(true_classes, predicted_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_pred(vgg16_model,5 , X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrong_pred_image(X_test, true_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correct_pred_image(X_test, true_classes, predicted_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
